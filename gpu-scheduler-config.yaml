# GPU Scheduler Configuration for Flexible Allocation
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-scheduler-config
  namespace: jupyterhub
data:
  gpu_allocation.py: |
    """
    Dynamic GPU allocation based on user requests and availability
    """
    import os
    import subprocess
    import json
    
    def get_gpu_availability():
        """Get current GPU usage and availability"""
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
                                   '--format=csv,noheader,nounits'], 
                                   capture_output=True, text=True)
            gpus = []
            for line in result.stdout.strip().split('\n'):
                if line:
                    idx, mem_used, mem_total, util = line.split(', ')
                    gpus.append({
                        'index': int(idx),
                        'memory_used': int(mem_used),
                        'memory_total': int(mem_total),
                        'memory_free': int(mem_total) - int(mem_used),
                        'utilization': int(util),
                        'available': int(util) < 50  # Consider available if <50% utilized
                    })
            return gpus
        except Exception as e:
            print(f"Error getting GPU info: {e}")
            return []
    
    def allocate_gpus(requested_gpus, user_priority='normal'):
        """Allocate GPUs based on request and availability"""
        available_gpus = get_gpu_availability()
        allocated = []
        
        # Sort by availability (least used first)
        available_gpus.sort(key=lambda x: (x['utilization'], x['memory_used']))
        
        # Allocate requested number of GPUs
        for gpu in available_gpus:
            if len(allocated) >= requested_gpus:
                break
            if gpu['available'] or user_priority == 'high':
                allocated.append(gpu['index'])
        
        return allocated
    
    def get_recommended_batch_size(allocated_gpus):
        """Recommend batch size based on allocated GPU memory"""
        if not allocated_gpus:
            return 32  # CPU-only default
        
        # RTX 3090 has 24GB, can handle larger batches
        total_memory = sum(gpu['memory_total'] for gpu in allocated_gpus)
        
        if total_memory > 20000:  # >20GB
            return 128
        elif total_memory > 10000:  # >10GB
            return 64
        else:
            return 32
