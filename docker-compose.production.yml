version: '3.8'

services:
  # Nginx Reverse Proxy with SSL
  nginx:
    image: nginx:alpine
    container_name: ai-lab-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sites-enabled/ai-lab-ip.conf:/etc/nginx/sites-enabled/default.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./ai_lab_user_platform.html:/usr/share/nginx/html/index.html:ro
      - ./ai_lab_admin_portal.html:/usr/share/nginx/html/admin.html:ro
      - ./nginx/auth/htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      - backend
      - mlflow
      - grafana
      - prometheus
    networks:
      - ai-lab-network
    restart: unless-stopped

  # Backend API Service
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: ai-lab-backend
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/ailab
      - SECRET_KEY=${SECRET_KEY}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - DOCKER_HOST=unix:///var/run/docker.sock
      - NVIDIA_VISIBLE_DEVICES=all
      - HOST_DATA_PATH=${HOST_DATA_PATH}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./ai-lab-data:/app/ai-lab-data
      - gpu_allocations:/app/gpu_allocations
      - ./ai_lab_backend.py:/app/ai_lab_backend.py:ro
    networks:
      - ai-lab-network
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]
    restart: unless-stopped

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: ai-lab-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ailab
      POSTGRES_INITDB_ARGS: "-E UTF8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    networks:
      - ai-lab-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # MLflow Tracking Server
  mlflow:
    build:
      context: .
      dockerfile: Dockerfile.mlflow
    container_name: ai-lab-mlflow
    environment:
      BACKEND_STORE_URI: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/mlflowdb
      ARTIFACT_ROOT: /shared/mlflow-artifacts
      MLFLOW_EXPOSE_PROMETHEUS: "true"
    volumes:
      - mlflow_artifacts:/shared/mlflow-artifacts
    networks:
      - ai-lab-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Prometheus for Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: ai-lab-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./monitoring/prometheus-prod.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - ai-lab-network
    restart: unless-stopped

  # Grafana for Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: ai-lab-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_ADMIN_USER: admin
      GF_SERVER_ROOT_URL: https://${DOMAIN}/grafana
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Viewer"
      GF_AUTH_ANONYMOUS_HIDE_VERSION: "true"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_USERS_ALLOW_ORG_CREATE: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - ai-lab-network
    depends_on:
      - prometheus
    restart: unless-stopped

  # TorchServe for Model Serving
  torchserve:
    build:
      context: .
      dockerfile: Dockerfile.torchserve
    container_name: ai-lab-torchserve
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
      TS_METRICS_MODE: prometheus
    volumes:
      - model_store:/mnt/models
      - mlflow_artifacts:/mnt/mlflow-artifacts:ro
      - ./torchserve-config.properties:/home/model-server/config.properties:ro
    networks:
      - ai-lab-network
    depends_on:
      mlflow:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Redis for Session Management
  redis:
    image: redis:7-alpine
    container_name: ai-lab-redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - ai-lab-network
    restart: unless-stopped

  # NVIDIA GPU Monitoring
  nvidia-dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    container_name: ai-lab-gpu-monitor
    environment:
      NVIDIA_VISIBLE_DEVICES: all
    volumes:
      - /run/nvidia:/run/nvidia:ro
    networks:
      - ai-lab-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  postgres_data:
    driver: local
  mlflow_artifacts:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/mlflow-artifacts
  grafana_data:
    driver: local
  prometheus_data:
    driver: local
  model_store:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/model-store
  redis_data:
    driver: local
  gpu_allocations:
    driver: local

networks:
  ai-lab-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16 