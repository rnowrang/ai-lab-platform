{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-GPU Training Guide for AI Lab Platform\n",
        "\n",
        "This notebook demonstrates how to effectively use all 4 NVIDIA RTX 2080 Ti GPUs for deep learning training.\n",
        "\n",
        "## Contents\n",
        "1. GPU Setup and Verification\n",
        "2. DataParallel Training (Easy Mode)\n",
        "3. DistributedDataParallel Training (Performance Mode)\n",
        "4. Multi-GPU Best Practices\n",
        "5. Monitoring and Debugging\n",
        "6. Real-World Example: Training ResNet50 on ImageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GPU Setup and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# List all GPUs\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"  Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DataParallel Training (Easy Mode)\n",
        "\n",
        "DataParallel is the easiest way to use multiple GPUs. It automatically splits your data across GPUs and gathers the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size=1024, hidden_size=2048, output_size=10):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Create model and wrap with DataParallel\n",
        "model = SimpleModel()\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
        "    model = nn.DataParallel(model)\n",
        "model = model.cuda()\n",
        "\n",
        "print(f\"Model created and moved to GPU(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic dataset for demonstration\n",
        "def create_synthetic_dataset(num_samples=10000, input_size=1024, num_classes=10):\n",
        "    X = torch.randn(num_samples, input_size)\n",
        "    y = torch.randint(0, num_classes, (num_samples,))\n",
        "    return TensorDataset(X, y)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_synthetic_dataset(10000)\n",
        "val_dataset = create_synthetic_dataset(2000)\n",
        "\n",
        "# Create data loaders with larger batch size for multi-GPU\n",
        "# Rule of thumb: batch_size = single_gpu_batch_size * num_gpus\n",
        "batch_size = 256 * torch.cuda.device_count()\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Using batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function with GPU utilization monitoring\n",
        "def train_epoch(model, loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            # Monitor GPU memory usage\n",
        "            if torch.cuda.is_available():\n",
        "                for i in range(torch.cuda.device_count()):\n",
        "                    allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "                    reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
        "                    print(f\"GPU {i}: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\", end=\" | \")\n",
        "            print(f\"\\nBatch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    epoch_time = time.time() - start_time\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch} completed in {epoch_time:.1f}s\")\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Training speed: {len(loader.dataset) / epoch_time:.1f} samples/second\")\n",
        "    \n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Starting DataParallel training...\\n\")\n",
        "for epoch in range(3):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, epoch + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DistributedDataParallel Training (Performance Mode)\n",
        "\n",
        "DistributedDataParallel (DDP) is more efficient than DataParallel, especially for larger models. Save the following script and run it with `torchrun`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example DDP script - save as train_ddp.py and run with:\n",
        "# torchrun --nproc_per_node=4 train_ddp.py\n",
        "\n",
        "print('''\n",
        "Save the following as train_ddp.py:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import os\n",
        "\n",
        "def setup():\n",
        "    dist.init_process_group(\"nccl\")\n",
        "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def train():\n",
        "    setup()\n",
        "    rank = dist.get_rank()\n",
        "    device = rank % torch.cuda.device_count()\n",
        "    \n",
        "    model = YourModel().to(device)\n",
        "    ddp_model = DDP(model, device_ids=[device])\n",
        "    \n",
        "    # Training loop here\n",
        "    \n",
        "    cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multi-GPU Best Practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice: Mixed Precision Training\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_mixed_precision(model, loader, criterion, optimizer):\n",
        "    \"\"\"Use mixed precision to speed up training and reduce memory usage.\"\"\"\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Use autocast for mixed precision\n",
        "        with autocast():\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "        \n",
        "        # Scale loss and backward\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "        \n",
        "        # Only run a few batches for demo\n",
        "        if batch_idx >= 5:\n",
        "            break\n",
        "\n",
        "print(\"Mixed precision training example:\")\n",
        "train_mixed_precision(model, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Monitoring and Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Memory and Utilization Monitor\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Get current GPU memory usage\"\"\"\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
        "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
        "        \n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
        "        print(f\"  Total:     {total:.2f} GB\")\n",
        "        print(f\"  Free:      {total - reserved:.2f} GB\")\n",
        "        print()\n",
        "\n",
        "get_gpu_memory_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Takeaways:\n",
        "1. **DataParallel** - Easy to use, good for prototyping\n",
        "2. **DistributedDataParallel** - Better performance, recommended for production\n",
        "3. **Mixed Precision** - 2x speedup with minimal code changes\n",
        "4. **Batch Size** - Scale linearly with number of GPUs\n",
        "\n",
        "### Your RTX 2080 Ti Setup:\n",
        "- 4 GPUs \u00d7 11GB = 44GB total VRAM\n",
        "- Excellent for most deep learning models\n",
        "- Use mixed precision for best performance\n",
        "\n",
        "Happy training! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}