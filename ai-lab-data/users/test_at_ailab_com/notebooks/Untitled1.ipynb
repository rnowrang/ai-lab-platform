{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104ccae7-3e39-4449-a41b-b4d8c3af2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d516f5-bba8-4c16-a0c1-fa134c0e2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 CUDA device(s).\n",
      "6fdef14527a4:115:115 [0] NCCL INFO cudaDriverVersion 12000\n",
      "6fdef14527a4:115:115 [0] NCCL INFO Bootstrap: Using eth0:172.20.0.11<0>\n",
      "6fdef14527a4:115:115 [0] NCCL INFO NCCL version 2.26.2+cuda12.2\n",
      "6fdef14527a4:115:195 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.\n",
      "6fdef14527a4:115:195 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "6fdef14527a4:115:195 [0] NCCL INFO NET/Socket : Using [0]eth0:172.20.0.11<0>\n",
      "6fdef14527a4:115:195 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "6fdef14527a4:115:195 [0] NCCL INFO Using network Socket\n",
      "6fdef14527a4:115:196 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "6fdef14527a4:115:196 [1] NCCL INFO Using network Socket\n",
      "6fdef14527a4:115:197 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "6fdef14527a4:115:197 [2] NCCL INFO Using network Socket\n",
      "6fdef14527a4:115:198 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n",
      "6fdef14527a4:115:198 [3] NCCL INFO Using network Socket\n",
      "6fdef14527a4:115:195 [0] NCCL INFO ncclCommInitAll comm 0x55df9d1be410 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3b000 commId 0xf524faca76451b28 - Init START\n",
      "6fdef14527a4:115:197 [2] NCCL INFO ncclCommInitAll comm 0x55df9d2b5b30 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 86000 commId 0xf524faca76451b28 - Init START\n",
      "6fdef14527a4:115:196 [1] NCCL INFO ncclCommInitAll comm 0x55df9d239fa0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5e000 commId 0xf524faca76451b28 - Init START\n",
      "6fdef14527a4:115:198 [3] NCCL INFO ncclCommInitAll comm 0x55df9d3316c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId d8000 commId 0xf524faca76451b28 - Init START\n",
      "6fdef14527a4:115:196 [1] NCCL INFO RAS client listening socket at ::1<28028>\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Bootstrap timings total 0.002761 (create 0.000108, send 0.000343, recv 0.000674, ring 0.000212, delay 0.000001)\n",
      "6fdef14527a4:115:197 [2] NCCL INFO Bootstrap timings total 0.002613 (create 0.000113, send 0.000335, recv 0.001114, ring 0.000237, delay 0.000000)\n",
      "6fdef14527a4:115:196 [1] NCCL INFO Bootstrap timings total 0.002491 (create 0.000102, send 0.000309, recv 0.000680, ring 0.000266, delay 0.000000)\n",
      "6fdef14527a4:115:198 [3] NCCL INFO Bootstrap timings total 0.002418 (create 0.000103, send 0.000300, recv 0.001146, ring 0.000206, delay 0.000000)\n",
      "6fdef14527a4:115:197 [2] NCCL INFO NCCL_P2P_DISABLE set by environment to 1\n",
      "6fdef14527a4:115:197 [2] NCCL INFO Setting affinity for GPU 2 to ff00ff00\n",
      "6fdef14527a4:115:196 [1] NCCL INFO Setting affinity for GPU 1 to ff00ff\n",
      "6fdef14527a4:115:197 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "6fdef14527a4:115:196 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "6fdef14527a4:115:198 [3] NCCL INFO Setting affinity for GPU 3 to ff00ff00\n",
      "6fdef14527a4:115:198 [3] NCCL INFO NVLS multicast support is not available on dev 3\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Setting affinity for GPU 0 to ff00ff\n",
      "6fdef14527a4:115:195 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "6fdef14527a4:115:198 [3] NCCL INFO comm 0x55df9d3316c0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0\n",
      "6fdef14527a4:115:198 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\n",
      "6fdef14527a4:115:198 [3] NCCL INFO P2P Chunksize set to 131072\n",
      "6fdef14527a4:115:197 [2] NCCL INFO comm 0x55df9d2b5b30 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0\n",
      "6fdef14527a4:115:197 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\n",
      "6fdef14527a4:115:197 [2] NCCL INFO P2P Chunksize set to 131072\n",
      "6fdef14527a4:115:195 [0] NCCL INFO comm 0x55df9d1be410 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\n",
      "6fdef14527a4:115:196 [1] NCCL INFO comm 0x55df9d239fa0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Channel 00/02 : 0 1 2 3\n",
      "6fdef14527a4:115:196 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\n",
      "6fdef14527a4:115:196 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Channel 01/02 : 0 1 2 3\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "6fdef14527a4:115:195 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 1\n",
      "6fdef14527a4:115:201 [2] NCCL INFO [Proxy Service] Device 2 CPU core 31\n",
      "6fdef14527a4:115:204 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 27\n",
      "6fdef14527a4:115:200 [3] NCCL INFO [Proxy Service] Device 3 CPU core 28\n",
      "6fdef14527a4:115:203 [1] NCCL INFO [Proxy Service] Device 1 CPU core 2\n",
      "6fdef14527a4:115:202 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 8\n",
      "6fdef14527a4:115:206 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 20\n",
      "6fdef14527a4:115:205 [0] NCCL INFO [Proxy Service] Device 0 CPU core 19\n",
      "6fdef14527a4:115:207 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5\n",
      "6fdef14527a4:115:198 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "6fdef14527a4:115:198 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "6fdef14527a4:115:197 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "6fdef14527a4:115:197 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "6fdef14527a4:115:196 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "6fdef14527a4:115:196 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "6fdef14527a4:115:195 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "6fdef14527a4:115:195 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "6fdef14527a4:115:195 [0] NCCL INFO CC Off, workFifoBytes 1048576\n",
      "6fdef14527a4:115:198 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n",
      "6fdef14527a4:115:198 [3] NCCL INFO ncclCommInitAll comm 0x55df9d3316c0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId d8000 commId 0xf524faca76451b28 - Init COMPLETE\n",
      "6fdef14527a4:115:198 [3] NCCL INFO Init timings - ncclCommInitAll: rank 3 nranks 4 total 0.07 (kernels 0.00, alloc 0.01, bootstrap 0.00, allgathers 0.01, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\n",
      "6fdef14527a4:115:197 [2] NCCL INFO ncclCommInitAll comm 0x55df9d2b5b30 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 86000 commId 0xf524faca76451b28 - Init COMPLETE\n",
      "6fdef14527a4:115:197 [2] NCCL INFO Init timings - ncclCommInitAll: rank 2 nranks 4 total 0.07 (kernels 0.00, alloc 0.01, bootstrap 0.00, allgathers 0.01, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\n",
      "6fdef14527a4:115:195 [0] NCCL INFO ncclCommInitAll comm 0x55df9d1be410 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3b000 commId 0xf524faca76451b28 - Init COMPLETE\n",
      "6fdef14527a4:115:196 [1] NCCL INFO ncclCommInitAll comm 0x55df9d239fa0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5e000 commId 0xf524faca76451b28 - Init COMPLETE\n",
      "6fdef14527a4:115:195 [0] NCCL INFO Init timings - ncclCommInitAll: rank 0 nranks 4 total 0.07 (kernels 0.00, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.04, graphs 0.00, connections 0.01, rest 0.00)\n",
      "6fdef14527a4:115:196 [1] NCCL INFO Init timings - ncclCommInitAll: rank 1 nranks 4 total 0.07 (kernels 0.00, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\n",
      "\n",
      "[2025-06-10 00:06:31] 6fdef14527a4:115:205 [0] misc/shmutils.cc:87 NCCL WARN Error: failed to extend /dev/shm/nccl-q68Hfo to 9637892 bytes, error: No space left on device (28)\n",
      "\n",
      "[2025-06-10 00:06:31] 6fdef14527a4:115:205 [0] misc/shmutils.cc:129 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-q68Hfo (size 9637888), error: No space left on device (28)\n",
      "6fdef14527a4:115:205 [0] NCCL INFO transport/shm.cc:549 -> 2\n",
      "6fdef14527a4:115:205 [0] NCCL INFO transport/shm.cc:491 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO transport/shm.cc:153 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO transport.cc:35 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO transport.cc:147 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO transport/generic.cc:19 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO group.cc:148 -> 2\n",
      "6fdef14527a4:115:211 [0] NCCL INFO group.cc:75 -> 2 [Async thread]\n",
      "6fde"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 64\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     66\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 193\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[T]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:126\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    124\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    125\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m--> 126\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m    129\u001b[0m buffers_rg: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:95\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m \u001b[43mBroadcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     97\u001b[0m             tensor_copies[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m     99\u001b[0m         ]\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/comm.py:66\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     65\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    \"\"\"\n",
    "    Calls nvidia-smi to query each GPU's compute and memory utilization,\n",
    "    then prints it out.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query index, GPU util (%), memory util (%) in CSV format without units\n",
    "        cmd = [\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=index,utilization.gpu,utilization.memory\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ]\n",
    "        output = subprocess.check_output(cmd).decode().strip()\n",
    "        for line in output.splitlines():\n",
    "            idx, gpu_util, mem_util = [x.strip() for x in line.split(\",\")]\n",
    "            print(f\"  GPU {idx}: {gpu_util}% GPU util, {mem_util}% memory util\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying nvidia-smi: {e}\")\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    \"\"\"Tiny feed-forward net to generate some GPU load.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def main():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No CUDA devices found.\")\n",
    "        return\n",
    "\n",
    "    ngpu = torch.cuda.device_count()\n",
    "    print(f\"Found {ngpu} CUDA device(s).\")\n",
    "\n",
    "    # Wrap model in DataParallel to shard batches across all GPUs\n",
    "    model = DummyModel().cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Create dummy data sized to keep GPUs busy\n",
    "    batch_size = 64\n",
    "    input_dim = 1024\n",
    "    data = torch.randn(batch_size, input_dim, device='cuda')\n",
    "    target = torch.randn(batch_size, input_dim, device='cuda')\n",
    "\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} — loss: {loss.item():.4f}\")\n",
    "        print(\"GPU utilization:\")\n",
    "        print_gpu_utilization()\n",
    "\n",
    "        # Pause so you can watch the numbers\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36771263-829f-4815-a9ab-2a383a936941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
